{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef0e76f0-968b-4d90-a157-5a564fd9d2a5",
   "metadata": {},
   "source": [
    "# Generate Nextflow Pipeline JobScript for Biomarker GWAS - 2\n",
    "\n",
    "This notebook creates the jobscript for running the longitudinal GWAS pipeline. It depends on the following files:\n",
    "\n",
    "- `nextflow/jobs.ini` -> configuration file that defines the paths of generic pipeline inputs\n",
    "- `nextflow/params/analysis-params.json` -> json file that defines the parameters and paths of inputs for GWAS analysis\n",
    "\n",
    "### `nextflow/jobs.ini`\n",
    "\n",
    "Modify the parameters in the `[Pipeline]` section for your respective project\n",
    "\n",
    "- `script` is the nextflow script for the pipeline (can use the default)\n",
    "- `config` is the config file for the pipeline\n",
    "- `logPath` defines the logging path for each analysis\n",
    "- `chunkSize` defines the size of each chunk for preprocessing step\n",
    "- `workDir` location of nextflow work directory\n",
    "\n",
    "Keep the paths for the datasets the same in the `[ADNI]` and `[AMP-PD]` sections. These section names should correspond to the keys in the `nextflow/params/analysis-params.json` file.\n",
    "\n",
    "### `nextflow/nextflow.config`\n",
    "\n",
    "Modify the `OUTPUT_DIR` variable in this file to set the output path for the pipeline results\n",
    "\n",
    "### `nextflow/params/analysis-params.json`\n",
    "\n",
    "Structure of the file should be\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"covariates\": [\"sex\", \"age\", \"PC1\", \"PC2\", \"PC3\"], // pipeline covariates to include in the model (aside from PC1-10 should be present in covarfiles\n",
    "  \"AMP-PD\": {   // key corresponds dataset in jobs.ini\n",
    "    \"lt\": {     // [\"lt\", \"cs\"] defines longitudinal or cross-sectional analysis\n",
    "      \"basePath\": \"/data/CARD/projects/longGWASnextflow/ExampleGWAS/input\", // this defines the base input paths for analysis defined below\n",
    "                                                                            // the basePath is appended to the covarfile and phenofile\n",
    "      \"out_sfx\": \"-PPMI-EX-LT\",                           // suffix for this combination of covariates and dataset - if not unique uses existing cache\n",
    "      \"analysis\": [                                       // list of analysis to be run\n",
    "        {\n",
    "          \"outcome\": \"phenotype_1\",                       // name of the outcome variable in the phenofile\n",
    "          \"covarfile\": \"path/to/input_covariates.tsv\",    // relative path from the basePath variable above to the covariates\n",
    "          \"phenofile\": \"path/to/input_phenotypes_1.tsv\"   // relative path from the basePath variable above to the outcome\n",
    "        },\n",
    "        {\n",
    "          \"outcome\": \"phenotype_2\",\n",
    "          \"covarfile\": \"path/to/input_covariates.tsv\",\n",
    "          \"phenofile\": \"path/to/input_phenotypes_2.tsv\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "### Directories\n",
    "\n",
    "the following directories should exist relative to project folder, in this case `ExampleGWAS`\n",
    "\n",
    "- `./jobs`\n",
    "- `./logs`\n",
    "- `./nextflow/params`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc282857-724f-46ed-88f7-891e9eea9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ed1977-f07f-4691-ba37-9e7501fe3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_job(sbatch_params, job_params):\n",
    "  header = \"#! /bin/bash\\n\"\n",
    "  for key, value in sbatch_params.items():\n",
    "    header += f'#SBATCH {key}={value}\\n'\n",
    "\n",
    "  modules = \"\"\"\\\n",
    "module load nextflow/20.01.0\n",
    "module load singularity\n",
    "\"\"\"\n",
    "\n",
    "  env_vars = f\"\"\"\\\n",
    "export TMPDIR=/lscratch/$SLURM_JOB_ID\n",
    "outcome=\"{job_params['outcome']}\"\n",
    "mode=\"{job_params['mode']}\"\n",
    "dataset=\"{job_params['dataset']}\"\n",
    "\"\"\"\n",
    "\n",
    "  env_setup = \"\"\"\\\n",
    "for i in $(seq 1 22); do\n",
    "\"\"\"\n",
    "\n",
    "  if meta_params['p1_cache']:\n",
    "    env_setup += \"  touch /lscratch/$SLURM_JOB_ID/chr${i}.vcf.gz\"\n",
    "  else:\n",
    "    env_setup += f\" cp {job_params['data-path']}\" + \" /lscratch/$SLURM_JOB_ID/chr${i}.vcf.gz\"\n",
    "\n",
    "  env_setup += f\"\"\"\n",
    "done\n",
    "\n",
    "echo \"Files in /lscratch/$SLURM_JOB_ID:\"\n",
    "ls /lscratch/$SLURM_JOB_ID\n",
    "\n",
    "ulimit -Sv {sbatch_params['--mem'][:-1]}000000\n",
    "\"\"\"\n",
    "\n",
    "  job = f\"\"\"\\\n",
    "srun nextflow -log {job_params['log-path']} run {job_params['script-path']} \\\\\n",
    "  -w \"{job_params['work-dir']}\" \\\\\n",
    "  --input \"{job_params['input']}\" \\\\\n",
    "  --dataset \"{job_params['dataset']}\" \\\\\n",
    "  --chunk {job_params['chunk_size']} \\\\\n",
    "  --minor_allele_ct {job_params['MAC']} \\\\\n",
    "  --out {job_params['out_fmt']} \\\\\n",
    "  --covarfile {job_params['covar-path']} \\\\\n",
    "  --phenofile {job_params['pheno-path']} \\\\\n",
    "  --ancestry \"{job_params['ancestry']}\" \\\\\n",
    "  --pheno_name \"{job_params['outcome']}\" \\\\\n",
    "  --chunk_flag \\\\\n",
    "  --covariates \"{job_params['covariates']}\" \"\"\"\n",
    "\n",
    "  if job_params['mode'] == 'lt':\n",
    "    job += f\"\"\"\\\\\n",
    "  --time_col \"{job_params['time_col']}\" \\\\\n",
    "  --longitudinal_flag \n",
    "\"\"\"\n",
    "\n",
    "  if job_params['mh_plot']:\n",
    "    job += \"\"\"\\\\\n",
    "  --mh_plot \n",
    "\"\"\"\n",
    "    \n",
    "  return '\\n'.join([header, modules, env_vars, env_setup, job])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce6bc12-657f-40d5-a8a3-1fbe1cab2c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dir = Path('./nextflow/params')\n",
    "analysis = {\n",
    "  'AMP-PD/woAAO': params_dir / 'analysis-params.woAAO.json',\n",
    "  'AMP-PD/wAAO': params_dir / 'analysis-params.json',\n",
    "  'AMP-PD/cs': params_dir / 'analysis-params.cs.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c6c70c5-90c5-4665-bd04-2114a1a55e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_fp = analysis['AMP-PD/cs']\n",
    "tmp_fh = open(tmp_fp, 'r')\n",
    "aparams = json.loads(tmp_fh.read())\n",
    "tmp_fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a797775-1985-4d69-83cc-85145edb0539",
   "metadata": {},
   "source": [
    "Replace paths for \n",
    "`/data/CARD/projects/longGWASnextflow/ExampleGWAS` to your project path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "970358dc-e817-4797-ac51-dbc6a3aa9948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['50837973'] AMP-PD-BF-CS cs log_CSF_tTau\n",
      "['50837987'] AMP-PD-BF-CS cs log_CSF_pTau\n"
     ]
    }
   ],
   "source": [
    "MAC = 5\n",
    "mh_plot = False\n",
    "ancestry = 'EUR'\n",
    "reqs = {\n",
    "  'ram': 125,\n",
    "  'scratch': 250,\n",
    "  'cpus': 20,\n",
    "  'time': {'lt': '6:00:00',\n",
    "           'cs': '4:00:00'}\n",
    "}\n",
    "\n",
    "meta_params = {\n",
    "  'p1_cache': True,\n",
    "  'dataset': None,\n",
    "  'config_path': 'nextflow/jobs.ini'\n",
    "}\n",
    "\n",
    "run = True  # set to False to generate scripts but not submit jobs, set to True to execute jobs\n",
    "             # check that cache is available prior to submitting multiple jobs\n",
    "\n",
    "pipeline_config = configparser.ConfigParser()\n",
    "pipeline_config.read(meta_params['config_path'])\n",
    "\n",
    "# set lambda x: x == 'ADNI'\n",
    "for ds in filter(lambda x: x != 'covariates', aparams.keys()):\n",
    "  covariates = aparams['covariates']\n",
    "  for mode in filter( lambda x: True, aparams[ds].keys()):\n",
    "    out_sfx = aparams[ds][mode]['out_sfx']\n",
    "    time_col = aparams[ds][mode]['time_col'] if mode == 'lt' else None\n",
    "      \n",
    "    for jp in aparams[ds][mode]['analysis']:\n",
    "      if jp['outcome'] == 'log_CSF_Ab':\n",
    "        continue\n",
    "      covarfile = os.path.join(aparams[ds][mode]['basePath'],\n",
    "                               jp['covarfile'])\n",
    "      phenofile = os.path.join(aparams[ds][mode]['basePath'],\n",
    "                               jp['phenofile'])\n",
    "      outcome = jp['outcome']\n",
    "    \n",
    "      \n",
    "      meta_params['dataset'] = ds\n",
    "\n",
    "      assert(os.path.exists(covarfile))\n",
    "      assert(os.path.exists(phenofile))\n",
    "      assert(covarfile != phenofile)\n",
    "\n",
    "\n",
    "      ## job params\n",
    "      job_params = {\n",
    "        'data-path': pipeline_config[meta_params['dataset']]['dataPath'],\n",
    "        'log-path': pipeline_config['Pipeline']['logPath'],\n",
    "        'script-path': pipeline_config['Pipeline']['script'],\n",
    "        'dataset': meta_params['dataset'],\n",
    "        'input': \"/lsratch/${SLURM_JOB_ID}/chr*.vcf.gz\",\n",
    "        'work-dir': pipeline_config['Pipeline']['workDir'],\n",
    "        'chunk_size': pipeline_config['Pipeline']['chunkSize'],\n",
    "        'covariates': ' '.join(covariates),\n",
    "        'MAC': MAC,\n",
    "        'ancestry': ancestry,\n",
    "        'mode': mode,\n",
    "        'outcome': outcome,\n",
    "        'out_fmt': f\"{meta_params['dataset']}{out_sfx}\",\n",
    "        'covar-path': covarfile,\n",
    "        'pheno-path': phenofile,\n",
    "        'mh_plot': mh_plot,\n",
    "        'time_col': time_col\n",
    "      }\n",
    "\n",
    "      assert(os.path.exists(job_params['script-path']))\n",
    "      \n",
    "      ## SBATCH params\n",
    "      sbatch_params = {\n",
    "        '--cpus-per-task': reqs['cpus'],\n",
    "        '--mem': f\"{reqs['ram']}g\",\n",
    "        '--gres': f\"lscratch:{reqs['scratch']}\",\n",
    "        '--time': f\"{reqs['time'][mode]}\",\n",
    "        '--partition': 'norm,quick'\n",
    "      }\n",
    "      \n",
    "      cmd = construct_job(sbatch_params, job_params)\n",
    "      \n",
    "      job_path = \"jobs\"\n",
    "      job_path = os.path.join( job_path,\n",
    "                               f\"jobscript.{meta_params['dataset']}-{outcome}.{mode}.bat\")\n",
    "      with open(job_path, 'w') as f:\n",
    "        f.write( cmd )\n",
    "        \n",
    "        \n",
    "      call_cmd = ['sbatch']\n",
    "      call_cmd += ['--out', f\"logs/{job_params['out_fmt']}.{outcome}.out\"]\n",
    "      call_cmd += ['--error', f\"logs/{job_params['out_fmt']}.{outcome}.err\"]\n",
    "      call_cmd += [job_path]\n",
    "\n",
    "      if run:\n",
    "        job_id =subprocess.run(call_cmd,\n",
    "                                   stdout=subprocess.PIPE).stdout.decode('utf-8').strip().split('\\n')\n",
    "        print(job_id, job_params['out_fmt'], mode, outcome)\n",
    "        time.sleep(5)\n",
    "      else:\n",
    "        print(job_params['out_fmt'], mode, outcome)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python/3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
