{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ab1cb4-30e8-424d-a43f-9ed27e0fd21b",
   "metadata": {},
   "source": [
    "# Risk loci heterogeneity analysis\n",
    "\n",
    "Dataframe key `AD.risk_loci` contains the risk loci from Bellenguez et.al. (columns: rsID, POS)\n",
    "\n",
    "Outcomes\n",
    "- `log CSF AÎ²`\n",
    "- `log CSF p-tau`\n",
    "- `log CSF t-tau`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5744e3a7-5954-42c0-99e8-0e9b6df789ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tabix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d669c5f-f5f0-45f4-a223-d87c59ce80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\n",
    "  'AD.risk_loci': pd.read_csv('METAL/RiskLoci/AD_loci.tsv',\n",
    "                             sep='\\t'),\n",
    "  'PD.risk_loci': pd.read_csv('archive/META5_rsID_hg38.csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948862f5-9c24-409b-9c76-721b79c2d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for oc in ['log_CSF_Ab', 'log_CSF_pTau', 'log_CSF_tTau']:\n",
    "  dfs[f'ADNI-Dementia.{oc}'] = tabix.open(f'METAL/raw_summary_stats/lt/EUR_ADNI-Dementia_allchr.{oc}.gallop.gz')\n",
    "  dfs[f'HC.{oc}'] = pd.read_csv(f'METAL/meta_analysis/lt/{oc}/Pi_HC1.tbl', sep='\\t')\n",
    "  dfs[f'PD.{oc}'] = pd.read_csv(f'METAL/meta_analysis/lt/{oc}/Pi_PD.all1.tbl', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64b299-d191-4bca-9dd3-8c7cad84f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d45c25b-8f00-4cfb-bda0-58e16544613b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_df = pd.read_csv('PD_omic_df.csv')\n",
    "tmp_df = tmp_df[tmp_df.p_GWAS < 5e-8][['topSNP_chr', 'topSNP_bp', 'A1', 'A2', 'topRSID']]\\\n",
    "            .drop_duplicates().reset_index(drop=True).copy()\n",
    "tmp_df['POS'] = 'chr' + tmp_df.topSNP_chr.astype(str) + ':' + tmp_df.topSNP_bp.astype(str)\n",
    "tmp_df['Allele'] = tmp_df.A1 + '/' + tmp_df.A2\n",
    "tmp_df = tmp_df.rename(columns={\n",
    "  'topRSID': 'rsID'\n",
    "})[['rsID', 'POS', 'Allele']]\n",
    "dfs['PD.risk_loci'] = tmp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f7bff-9f2e-49f1-b8f0-d8d3c7b1f776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_df = pd.read_csv('PD_omic_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b99c6371-5df1-4480-8737-c5afd6058fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_df = pd.read_csv('METAL/FUMA/MarkerNames.rsID_annotated-2.tsv',\n",
    "                    sep='\\t',\n",
    "                    names=['MarkerName', 'rsID'])\n",
    "dfs['PD.risk_loci'] = dfs['PD.risk_loci.orig'].merge(tmp_df, on='rsID', how='inner')\n",
    "dfs['PD.risk_loci']['POS'] = dfs['PD.risk_loci'].MarkerName.str.split(':').apply(lambda x: ':'.join(x[:2]))\n",
    "dfs['PD.risk_loci'] = dfs['PD.risk_loci'][['rsID', 'MarkerName', 'POS']].drop_duplicates()\n",
    "dfs['PD.risk_loci']['Allele'] = dfs['PD.risk_loci'].MarkerName.str.split(':').apply(lambda x: '/'.join(x[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "60af8562-7a3d-4e29-9600-52a0107617f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "header = [\n",
    "  'CROM',\n",
    "  'POS',\n",
    "  'ID',\n",
    "  'REF',\n",
    "  'ALT',\n",
    "  'A1',\n",
    "  'A1_FREQ',\n",
    "  'MISS_FREQ',\n",
    "  'OBS_CT',\n",
    "  'TEST',\n",
    "  'BETAs',\n",
    "  'SEs',\n",
    "  'T_STAT',\n",
    "  'Ps',\n",
    "  'OBS_CT_REP',\n",
    "  'BETAi',\n",
    "  'SEi',\n",
    "  'Pi',\n",
    "  'COV'\n",
    "]\n",
    "\n",
    "risk_key = 'PD.risk_loci'\n",
    "#for idx, row in dfs['AD.risk_loci'].iterrows():\n",
    "for oc in ['log_CSF_Ab', 'log_CSF_pTau', 'log_CSF_tTau']:\n",
    "#for oc in ['log_CSF_tTau']:\n",
    "  r = []\n",
    "  for idx, row in dfs[risk_key].iterrows():\n",
    "    chrom, pos = row.POS.split(':')\n",
    "    pos = int(pos)\n",
    "\n",
    "    query = dfs[f'ADNI-Dementia.{oc}'].query(chrom, pos-1, pos)\n",
    "    try:\n",
    "      r.append(\n",
    "        next(query))\n",
    "    except StopIteration:\n",
    "      r.append([pd.NA] * len(header) )\n",
    "\n",
    "  df = pd.DataFrame(r, columns=header)\n",
    "  df['study'] = 'ADNI-Dementia'\n",
    "  df['outcome'] = oc\n",
    "\n",
    "  r = [df[['ID', 'A1', 'REF', 'A1_FREQ', 'BETAi', 'SEi', 'Pi', 'OBS_CT', 'outcome', 'study']].rename(columns={\n",
    "    'ID': 'MarkerName',\n",
    "    'A1': 'Allele2',\n",
    "    'REF': 'A1',\n",
    "    'A1_FREQ': 'A1_FREQ',\n",
    "    'BETAi': 'BETA',\n",
    "    'SEi': 'SE',\n",
    "    'Pi': 'P'\n",
    "  }).copy()]\n",
    "\n",
    "\n",
    "  for c in ['HC', 'PD']:\n",
    "    key = f'{c}.{oc}'\n",
    "    idx = dfs[key].MarkerName.str.split(':').apply(lambda x: ':'.join(x[:2])).isin(dfs[risk_key].POS)\n",
    "    dfs[key]['study'] = c\n",
    "    dfs[key]['outcome'] = oc\n",
    "    tmp_df = dfs[key][idx].copy()\n",
    "    tmp_df = tmp_df.rename(columns=\n",
    "                          {'Effect': 'BETA',\n",
    "                           'StdErr': 'SE',\n",
    "                           'P-value': 'P',\n",
    "                           'OBS_CT_total': 'OBS_CT'})\n",
    "    r.append( tmp_df[['MarkerName', 'Allele1', 'Allele2', 'Freq1', 'BETA', 'SE', 'P', 'OBS_CT', 'outcome', 'study']].rename(columns={\n",
    "      'Allele1': 'A1',\n",
    "      'Freq1': 'A1_FREQ'}).copy() )\n",
    "    \n",
    "  df = pd.concat(r).reset_index(drop=True) \n",
    "  for comp in ['HCAD', 'HCPD', 'PDAD']:\n",
    "    key = f'{comp}.Pi_{oc}'\n",
    "    if key not in dfs:\n",
    "      dfs[key] = pd.read_csv(f'METAL/meta_analysis/lt/{oc}/Pi_status.{comp}1.tbl', sep='\\t')\n",
    "    idx = dfs[key].MarkerName.str.split(':').apply(lambda x: ':'.join(x[:2])).isin(dfs[risk_key].POS)\n",
    "    df = df.merge(dfs[key][idx][['MarkerName', 'HetISq']].rename(columns={'HetISq': f'HetISq_{comp}'}), how='outer')\n",
    "\n",
    "  df['HetISq'] = df[['HetISq_HCAD', 'HetISq_HCPD', 'HetISq_PDAD']].max(axis=1)\n",
    "  res.append(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "8c006525-f931-48e1-a7cf-9668fad290f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(res).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "9e120575-a1e7-45c2-a4ef-461434dbaee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_copy = df.copy()\n",
    "df = df_copy.copy() # run if redoing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b7772e4-b85a-4e1a-bd61-a0e343cfae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.to_numeric(df['A1_FREQ']) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "8e3c6e10-f22e-482d-9ef8-dbf32cb2a1d4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['PD', 'ADNI-Dementia', 'HC'], dtype='object')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [318]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tmp_alleles \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mA1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# flip A1 allele\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllele2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.8/lib/python3.8/site-packages/pandas/core/indexing.py:961\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m    964\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.8/lib/python3.8/site-packages/pandas/core/indexing.py:1140\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1139\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_ellipsis(tup)\n\u001b[0;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.8/lib/python3.8/site-packages/pandas/core/indexing.py:891\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m section\n\u001b[1;32m    890\u001b[0m         \u001b[38;5;66;03m# This is an elided recursive call to iloc/loc\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot applicable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.8/lib/python3.8/site-packages/pandas/core/indexing.py:967\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    964\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    966\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.8/lib/python3.8/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.8/lib/python3.8/site-packages/pandas/core/indexing.py:1132\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1132\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1134\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.8/lib/python3.8/site-packages/pandas/core/indexing.py:1327\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1324\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1325\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1327\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py:5842\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5841\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['PD', 'ADNI-Dementia', 'HC'], dtype='object')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "tmp_alleles = df.loc[idx, 'A1'].copy()\n",
    "# flip A1 allele\n",
    "df.loc[idx, 'A1'] = df.loc[idx, 'Allele2']\n",
    "df.loc[idx, 'Allele2'] = tmp_alleles\n",
    "# flip BETA\n",
    "df.loc[idx, 'BETA'] = pd.to_numeric(df.loc[idx, 'BETA']) * -1.\n",
    "# create and update MAF\n",
    "df['MAF'] = df['A1_FREQ']\n",
    "df.loc[idx, 'MAF'] = 1. - pd.to_numeric(df.loc[idx, 'MAF'])\n",
    "df['A1'] = df.A1.str.upper()\n",
    "df['Allele2'] = df.Allele2.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "a4e6e157-98a4-471f-b750-7f0c80b3964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip alleles\n",
    "idx = df[~df.BETA.isna()].index\n",
    "df.loc[idx, 'BETA'] = df.loc[idx, 'BETA'].astype(float)\n",
    "\n",
    "for idx, grp in df.groupby('MarkerName'):\n",
    "  if grp.A1.nunique() > 1:\n",
    "    assert(grp[grp.study.isin(['HC', 'PD'])].A1.nunique() == 1)\n",
    "    test_allele = grp[grp.study.isin(['HC', 'PD'])].A1.unique()[0]\n",
    "    for rid, row in grp[grp.study == 'ADNI-Dementia'].iterrows():\n",
    "      if row.A1 != test_allele:\n",
    "        df.loc[row.name, 'Allele2'] = row.A1\n",
    "        df.loc[row.name, 'A1'] = test_allele\n",
    "        df.loc[row.name, 'BETA'] = row.BETA * -1.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "21792fc0-ba22-4929-a9aa-e28c686c169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back in `MAF`\n",
    "df[[\n",
    "  'MarkerName', 'A1', 'Allele2', ,\n",
    "  'BETA', 'SE', 'P', 'OBS_CT', 'outcome', 'study',\n",
    "  'HetISq_HCAD', 'HetISq_HCPD', 'HetISq_PDAD', 'HetISq'\n",
    "]].to_csv('METAL/RiskLoci/AD_risk_loci_het-new.tbl', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8b266-0b02-44cb-9a7e-603711dc913a",
   "metadata": {},
   "source": [
    "## Code for UPset Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8cd5ab1a-9adb-4c15-8648-8f674c6923f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\": \"dM3QQsP+wXdHg42wTqW85BjZQdLNNIXqlPw/BgKoExPmTG7ZLML4EGqLMfqHT6ON\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.3.min.js\": \"8x57I4YuIfu8XyZfFo0XVr2WAT8EK4rh/uDe3wF7YuW2FNUSNEpJbsPaB1nJ2fz2\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.3.min.js\": \"3QTqdz9LyAm2i0sG5XTePsHec3UHWwVsrOL68SYRoAXsafvfAyqtQ+h440+qIBhS\"};\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.3.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\": \"dM3QQsP+wXdHg42wTqW85BjZQdLNNIXqlPw/BgKoExPmTG7ZLML4EGqLMfqHT6ON\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.3.min.js\": \"8x57I4YuIfu8XyZfFo0XVr2WAT8EK4rh/uDe3wF7YuW2FNUSNEpJbsPaB1nJ2fz2\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.3.min.js\": \"3QTqdz9LyAm2i0sG5XTePsHec3UHWwVsrOL68SYRoAXsafvfAyqtQ+h440+qIBhS\"};\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.3.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import upsetplot\n",
    "from matplotlib import cm\n",
    "from upsetplot import plot, from_memberships\n",
    "from matplotlib import colors\n",
    "from matplotlib.tight_layout import get_renderer\n",
    "import matplotlib\n",
    "\n",
    "from scripts.MetaStats import MetalBrowser,MetalStats\n",
    "from scripts.SumStats import SumStats\n",
    "from scripts.ForestPlot import *\n",
    "from scripts.Filters import *\n",
    "\n",
    "from pathlib import Path\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure, output_file, save\n",
    "from itertools import combinations\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "87513883-53dc-4919-880c-85dd4cafd326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_matrix(self, ax, scaler, cmap):\n",
    "  \"\"\"Plot the matrix of intersection indicators onto ax\n",
    "  \"\"\"\n",
    "  ax = self._reorient(ax)\n",
    "  data = self.intersections\n",
    "  n_cats = data.index.nlevels\n",
    "  n_empty = len( list(filter(lambda x: x.strip() == '', data.index.names)) )\n",
    "  n_cats = n_cats - n_empty\n",
    "  label_cats = list( filter(lambda x: x.strip() != '', data.index.names) )\n",
    "  \n",
    "  inclusion = data.index.to_frame().values\n",
    "  mat_color = []\n",
    "  \n",
    "  # Prepare styling\n",
    "  styles = [\n",
    "    [\n",
    "        self.subset_styles[i]\n",
    "        #if inclusion[i, j]\n",
    "        #else {\"facecolor\": self._other_dots_color, \"linewidth\": 0}\n",
    "        for j in range(n_cats)\n",
    "    ]\n",
    "    for i in range(len(data))\n",
    "  ]\n",
    "\n",
    "  for i in range(len(data)):\n",
    "    for j in range(n_cats):\n",
    "      tmp_name = data.index.names[j]\n",
    "      if tmp_name.strip() == '':\n",
    "        continue\n",
    "      if i >= len(self.mat_color.loc[tmp_name]):\n",
    "        mat_color.append( None )\n",
    "        continue\n",
    "      mat_color.append( float(self.mat_color.loc[tmp_name].iloc[i]) )\n",
    "\n",
    "  styles = sum(styles, [])  # flatten nested list\n",
    "  style_columns = {\"facecolor\": \"facecolors\",\n",
    "                   \"edgecolor\": \"edgecolors\",\n",
    "                   \"linewidth\": \"linewidths\",\n",
    "                   \"linestyle\": \"linestyles\",\n",
    "                   \"hatch\": \"hatch\"}\n",
    "  styles = pd.DataFrame(styles).reindex(columns=style_columns.keys())\n",
    "\n",
    "  styles[\"linewidth\"] = self.inclusion.flatten() * 2\n",
    "  styles[\"linewidth\"] = styles[\"linewidth\"] + 1\n",
    "  styles[\"linewidth\"].fillna(0, inplace=True)\n",
    "  #styles[\"facecolor\"].fillna(self._facecolor, inplace=True)\n",
    "  styles[\"facecolor\"] = self._facecolor\n",
    "  styles[\"c\"] = mat_color\n",
    "  styles[\"edgecolor\"].fillna(styles[\"facecolor\"], inplace=True)\n",
    "  styles[\"edgecolor\"] = ['black' if x == 1 else 'gray' for x in self.inclusion.flatten()]\n",
    "  styles[\"linestyle\"].fillna(\"solid\", inplace=True)\n",
    "  del styles[\"hatch\"]  # not supported in matrix (currently)\n",
    "\n",
    "  x = np.repeat(np.arange(len(data)), n_cats)\n",
    "  y = np.tile(np.arange(n_cats), len(data))\n",
    "  # Plot dots\n",
    "  if self._element_size is not None:\n",
    "    s = (self._element_size * .35) ** 2\n",
    "  else:\n",
    "    # TODO: make s relative to colw\n",
    "    s = 200\n",
    "    \n",
    "  ax.scatter(*self._swapaxes(x, y), s=s, zorder=10, cmap=cmap,\n",
    "             vmin=0., vmax=1.,\n",
    "             **styles.rename(columns=style_columns))\n",
    "\n",
    "  ax.set_ylim( ax.get_ylim()[0], n_cats)\n",
    "  # Plot lines\n",
    "  if self._with_lines:\n",
    "    idx = np.flatnonzero(self.inclusion)\n",
    "    line_data = (pd.Series(y[idx], index=x[idx])\n",
    "                 .groupby(level=0)\n",
    "                 .aggregate(['min', 'max']))\n",
    "    colors = pd.Series([\n",
    "      style.get(\"edgecolor\", style.get(\"facecolor\", self._facecolor))\n",
    "      for style in self.subset_styles],\n",
    "      name=\"color\")\n",
    "    line_data = line_data.join(colors)\n",
    "    ax.vlines(line_data.index.values,\n",
    "              line_data['min'], line_data['max'],\n",
    "              lw=2, colors=line_data[\"color\"],\n",
    "              zorder=5)\n",
    "\n",
    "  # Ticks and axes\n",
    "  tick_axis = ax.yaxis\n",
    "  tick_axis.set_ticks(np.arange(n_cats))\n",
    "  tick_axis.set_ticklabels(label_cats,\n",
    "                           rotation=0 if self._horizontal else -90)\n",
    "  ax.xaxis.set_visible(True)\n",
    "  ax.tick_params(axis='both', which='both', length=0)\n",
    "\n",
    "  ax.xaxis.set_ticks( ticks=np.arange(len(data) + 1),\n",
    "                      labels=self.labels)\n",
    "                      #labels=[f'a-{i}' for i in range(n_cats -1)])\n",
    "  \n",
    "  \n",
    "  ax.set_xticklabels( self.labels, rotation=60, ha='right' )\n",
    "  if not self._horizontal:\n",
    "    ax.yaxis.set_ticks_position('top')\n",
    "  ax.set_frame_on(False)\n",
    "  #ax.set_xlim(-.5, x[-1] + .5, auto=False)\n",
    "  ax.grid(False)\n",
    "  \n",
    "  \n",
    "def plot_intersections(self, ax):\n",
    "  \"\"\"Plot bars indicating intersection size\n",
    "  \"\"\"\n",
    "  rects = self._plot_bars(ax, self.intersections,\n",
    "                          title='N Subjects',\n",
    "                          colors=self._facecolor)\n",
    "  for style, rect in zip(self.subset_styles, rects):\n",
    "    style = style.copy()\n",
    "    style.setdefault(\"edgecolor\",\n",
    "                     style.get(\"facecolor\", self._facecolor))\n",
    "    for attr, val in style.items():\n",
    "      getattr(rect, \"set_\" + attr)(val)\n",
    "\n",
    "  if self.subset_legend:\n",
    "    styles, labels = zip(*self.subset_legend)\n",
    "    styles = [patches.Patch(**patch_style) for patch_style in styles]\n",
    "    ax.legend(styles, labels)\n",
    "    \n",
    "    \n",
    "def make_grid(self, intersection, fig=None):\n",
    "      \"\"\"Get a SubplotSpec for each Axes, accounting for label text width\n",
    "      \"\"\"\n",
    "      data = self.intersections\n",
    "      n_cats = data.index.nlevels\n",
    "      n_empty = len( list(filter(lambda x: x.strip() == '', data.index.names)) )\n",
    "      n_cats = n_cats - n_empty\n",
    "      \n",
    "      \n",
    "      n_inters = len(data) + 2\n",
    "\n",
    "      if fig is None:\n",
    "          fig = plt.gcf()\n",
    "\n",
    "      # Determine text size to determine figure size / spacing\n",
    "      r = get_renderer(fig)\n",
    "      text_kw = {\"size\": matplotlib.rcParams['xtick.labelsize']}\n",
    "      # adding \"x\" ensures a margin\n",
    "      t = fig.text(0, 0, '\\n'.join(str(label) + \"x\"\n",
    "                                   for label in self.totals.index.values),\n",
    "                   **text_kw)\n",
    "      textw = t.get_window_extent(renderer=r).width\n",
    "      t.remove()\n",
    "\n",
    "      figw = self._reorient(fig.get_window_extent(renderer=r)).width\n",
    "\n",
    "      sizes = np.asarray([p['elements'] for p in self._subset_plots])\n",
    "      fig = self._reorient(fig)\n",
    "\n",
    "      non_text_nelems = len(self.intersections) + self._totals_plot_elements\n",
    "      if self._element_size is None:\n",
    "          colw = (figw - textw) / non_text_nelems\n",
    "      else:\n",
    "          render_ratio = figw / fig.get_figwidth()\n",
    "          colw = self._element_size / 72 * render_ratio\n",
    "          figw = colw * (non_text_nelems + np.ceil(textw / colw) + 1)\n",
    "          fig.set_figwidth(figw / render_ratio)\n",
    "          fig.set_figheight((colw * (n_cats + sizes.sum())) /\n",
    "                            render_ratio)\n",
    "\n",
    "      text_nelems = int(np.ceil(figw / colw - non_text_nelems))\n",
    "      # print('textw', textw, 'figw', figw, 'colw', colw,\n",
    "      #       'ncols', figw/colw, 'text_nelems', text_nelems)\n",
    "\n",
    "      GS = self._reorient(matplotlib.gridspec.GridSpec)\n",
    "      gridspec = GS(*self._swapaxes(n_cats + (sizes.sum() or 0),\n",
    "                                    n_inters + text_nelems +\n",
    "                                    self._totals_plot_elements),\n",
    "                    hspace=1)\n",
    "      if self._horizontal:\n",
    "          out = {'matrix': gridspec[-n_cats if intersection else 0:, -n_inters:-2],\n",
    "                 'shading': gridspec[-n_cats if intersection else 0:, :-2],\n",
    "                 'totals': gridspec[-n_cats:, :self._totals_plot_elements],\n",
    "                 'gs': gridspec,\n",
    "                 'cbar': gridspec[:, -1:]}\n",
    "          cumsizes = np.cumsum(sizes[::-1])\n",
    "          for start, stop, plot in zip(np.hstack([[0], cumsizes]), cumsizes,\n",
    "                                       self._subset_plots[::-1]):\n",
    "              out[plot['id']] = gridspec[start:stop, -n_inters:-2]\n",
    "      else:\n",
    "          out = {'matrix': gridspec[-n_inters:, :n_cats],\n",
    "                 'shading': gridspec[:, :n_cats],\n",
    "                 'totals': gridspec[:self._totals_plot_elements, :n_cats],\n",
    "                 'gs': gridspec}\n",
    "          cumsizes = np.cumsum(sizes)\n",
    "          for start, stop, plot in zip(np.hstack([[0], cumsizes]), cumsizes,\n",
    "                                       self._subset_plots):\n",
    "              out[plot['id']] = \\\n",
    "                  gridspec[-n_inters:, start + n_cats:stop + n_cats]\n",
    "      return out\n",
    "    \n",
    "def plot(self, scaler, cmap, cmap_fmt='{:.2e}', intersection=True, fig=None):\n",
    "        \"\"\"Draw all parts of the plot onto fig or a new figure\n",
    "        Parameters\n",
    "        ----------\n",
    "        fig : matplotlib.figure.Figure, optional\n",
    "            Defaults to a new figure.\n",
    "        Returns\n",
    "        -------\n",
    "        subplots : dict of matplotlib.axes.Axes\n",
    "            Keys are 'matrix', 'intersections', 'totals', 'shading'\n",
    "        \"\"\"\n",
    "        if fig is None:\n",
    "            fig = plt.figure(figsize=self._default_figsize)\n",
    "        specs = self.make_grid(intersection, fig)\n",
    "        shading_ax = fig.add_subplot(specs['shading'])\n",
    "        self.plot_shading(shading_ax)\n",
    "        matrix_ax = self._reorient(fig.add_subplot)(specs['matrix'],\n",
    "                                                    sharey=shading_ax)\n",
    "        cbar_ax = self._reorient(fig.add_subplot)(specs['cbar'])\n",
    "        cbar_ax.axis('off')\n",
    "        \n",
    "        self.plot_matrix(matrix_ax, scaler, cmap)\n",
    "        #totals_ax = self._reorient(fig.add_subplot)(specs['totals'],\n",
    "        #                                            sharey=matrix_ax)\n",
    "        #self.plot_totals(totals_ax)\n",
    "        out = {'matrix': matrix_ax,\n",
    "               'shading': shading_ax}\n",
    "               #'totals': totals_ax}\n",
    "\n",
    "        for plot in self._subset_plots:\n",
    "            if plot['type'] == 'default' and not intersection:\n",
    "              continue\n",
    "            ax = self._reorient(fig.add_subplot)(specs[plot['id']],\n",
    "                                                 sharex=matrix_ax)\n",
    "            \n",
    "            if plot['type'] == 'default':\n",
    "                self.plot_intersections(ax)\n",
    "                ax.set_ylabel('HetISq')\n",
    "            elif plot['type'] in self.PLOT_TYPES:\n",
    "                kw = plot.copy()\n",
    "                del kw['type']\n",
    "                del kw['elements']\n",
    "                del kw['id']\n",
    "                self.PLOT_TYPES[plot['type']](self, ax, **kw)\n",
    "            else:\n",
    "                raise ValueError('Unknown subset plot type: %r' % plot['type'])\n",
    "            out[plot['id']] = ax\n",
    "          \n",
    "        cbar = fig.colorbar(cm.ScalarMappable(norm=scaler, cmap=cmap), ax=cbar_ax,\n",
    "                            ticks=[0.5],\n",
    "                            fraction=1, pad=0.04)\n",
    "        #cbar.ax.set_yticklabels([scaler.inverse(0.1), scaler.inverse(.5), scaler.inverse(.9)])\n",
    "        cbar.ax.set_yticks([scaler.vmin, scaler.vcenter, scaler.vmax])\n",
    "        cbar.ax.set_yticklabels( map(lambda x: cmap_fmt.format(x), [scaler.vmin, scaler.vcenter, scaler.vmax]) )\n",
    "        return out\n",
    "\n",
    "upsetplot.UpSet.plot_matrix = plot_matrix\n",
    "upsetplot.UpSet.plot = plot\n",
    "upsetplot.UpSet.plot_intersections = plot_intersections\n",
    "upsetplot.UpSet.make_grid = make_grid\n",
    "\n",
    "Z_LOWER = -5.4513 # qnorm(5e-8 / 2)\n",
    "Z_UPPER = 5.4513\n",
    "CMAP_SCHEME = 'bwr_r'\n",
    "plt.rcParams.update({'font.size': 26})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "1a2ed2cf-d6e0-4fb8-9bc0-fcf9f0939015",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df[~df.BETA.isna()].index\n",
    "df.POS = ''\n",
    "df.loc[idx, 'POS'] = df.loc[idx, 'MarkerName'].str.split(':').apply(lambda x: ':'.join(x[:2]))\n",
    "df = df.merge(dfs['AD.risk_loci'][['rsID', 'POS', 'Allele']], on='POS')\n",
    "\n",
    "df['P'] = df.P.astype(float)\n",
    "idx = df.groupby('rsID').P.min().index\n",
    "df = df[df.rsID.isin(idx[(df.groupby('rsID').P.min() < 5e-2)])]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "d0f82f8f-712b-4944-a209-97685543bce8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rsID'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [224]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmerge(dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAD.risk_loci\u001b[39m\u001b[38;5;124m'\u001b[39m][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrsID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllele\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mP\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrsID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mP\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39mrsID\u001b[38;5;241m.\u001b[39misin(idx[(df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrsID\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mP\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5e-2\u001b[39m)])]\n\u001b[1;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.8/lib/python3.8/site-packages/pandas/core/frame.py:7718\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   7713\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m   7715\u001b[0m \u001b[38;5;66;03m# https://github.com/python/mypy/issues/7642\u001b[39;00m\n\u001b[1;32m   7716\u001b[0m \u001b[38;5;66;03m# error: Argument \"squeeze\" to \"DataFrameGroupBy\" has incompatible type\u001b[39;00m\n\u001b[1;32m   7717\u001b[0m \u001b[38;5;66;03m# \"Union[bool, NoDefault]\"; expected \"bool\"\u001b[39;00m\n\u001b[0;32m-> 7718\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   7719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7721\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7724\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7726\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   7727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7729\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.8/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:882\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[0;32m--> 882\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.8/lib/python3.8/site-packages/pandas/core/groupby/grouper.py:882\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    880\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 882\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rsID'"
     ]
    }
   ],
   "source": [
    "### OLD\n",
    "\n",
    "df['POS'] = df.MarkerName.str.split(':').apply(lambda x: ':'.join(x[:2]))\n",
    "df = df.merge(dfs['AD.risk_loci'][['rsID', 'POS', 'Allele']], on='POS')\n",
    "\n",
    "\n",
    "df['P'] = df.P.astype(float)\n",
    "idx = df.groupby('rsID').P.min().index\n",
    "df = df[df.rsID.isin(idx[(df.groupby('rsID').P.min() < 5e-2)])]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00510bea-738c-46f7-865c-6fa0e22eb062",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the allele flip here\n",
    "\n",
    "df['flip'] = df.A1.str.upper() != df.Allele.str.split('/', expand=True)[0]\n",
    "df.loc[df[df.flip].index,'BETA'] = df[df.flip]['BETA'].astype(float) * -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "5a00538c-8a0d-4b9f-92a3-c82818166016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Z'] = df['BETA'].astype(float) / df['SE'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "ceccae55-0e9d-4fc7-9c9a-afea73e1f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "oc = 'log_CSF_Ab'\n",
    "df = df[df.outcome==oc]\n",
    "df = df[df.HetISq > 0.].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "2f0d40b7-ecdb-415a-9c7a-2d38f0ce6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_input = []\n",
    "for idx, row in df[['MarkerName', 'rsID', 'outcome', 'HetISq']].drop_duplicates().iterrows():\n",
    "  for new_study in (' ', '  ', '   ', '    ', '     ')[:2]:\n",
    "    tmp_input.append(\n",
    "      [row[0], pd.NA, pd.NA, pd.NA, pd.NA, pd.NA, pd.NA, new_study, row[2], row[1], 0, row[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "fa3491dc-5a5d-432b-b24f-1cde7f4464ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54163/2360756797.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tmp_df = tmp_df.append(\n"
     ]
    }
   ],
   "source": [
    "tmp_df = df[['MarkerName', 'A1', 'Allele2', 'BETA', 'SE', 'P', 'OBS_CT', 'study', 'outcome', 'rsID', 'Z', 'HetISq']].copy()\n",
    "tmp_df = tmp_df.append(\n",
    "  pd.DataFrame(tmp_input,\n",
    "           columns=tmp_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "466518c5-82a5-402e-a958-48ca17741681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['intersection'] = pd.NA\n",
    "df.loc[np.where(df.HetISq_HCAD == df.HetISq)[0],'intersection'] = 'HCAD'\n",
    "df.loc[np.where(df.HetISq_HCPD == df.HetISq)[0],'intersection'] = 'HCPD'\n",
    "df.loc[np.where(df.HetISq_PDAD == df.HetISq)[0],'intersection'] = 'PDAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9dfb00-dc91-4f10-82ab-0b4ae753526f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_grp = list(combinations(tmp_df.study.unique(), 1)) + \\\n",
    "  list(combinations(tmp_df.study.unique(), 2)) + \\\n",
    "  list(combinations(tmp_df.study.unique(), 3)) + \\\n",
    "  list(combinations(tmp_df.study.unique(), 4)) + \\\n",
    "  list(combinations(tmp_df.study.unique(), 5)) + \\\n",
    "  list(combinations(tmp_df.study.unique(), 6))\n",
    "\n",
    "new_grp = []\n",
    "for x in tmp_grp:\n",
    "  if not set(map(str.strip, x)) == {''}:\n",
    "    new_grp.append(x)\n",
    "tmp_grp = new_grp\n",
    "tmp_grp = tmp_grp[:tmp_df.rsID.nunique()]\n",
    "tmp_grp = [list(x) for x in tmp_grp]\n",
    "\n",
    "grp = from_memberships(\n",
    "  tmp_grp,\n",
    "  data = tmp_df.groupby('rsID').HetISq.max().tolist())\n",
    "#  data=dfs.groupby(['outcome', 'rsID']).OBS_CT.sum().tolist())\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "a = upsetplot.UpSet(grp)\n",
    "\n",
    "tmp_data = pd.DataFrame( tmp_df.groupby('rsID').HetISq.first().sort_values(ascending=False) ).reset_index().set_index(a.intersections.index)\n",
    "tmp_data['outcome'] = oc\n",
    "\n",
    "idx = list(filter(lambda x: x.strip() != '', tmp_data.index.names))\n",
    "intersection_mapper = {\n",
    "  'HCAD': [1 if x != 'PD' else 0 for x in idx],\n",
    "  'HCPD': [1 if x != 'ADNI-Dementia' else 0 for x in idx],\n",
    "  'PDAD': [1 if x != 'HC' else 0 for x in idx],\n",
    "  pd.NA: [0 for x in idx]\n",
    "}\n",
    "inclusion_df = {\n",
    "  oc: tmp_df.merge(df[['study', 'rsID', 'intersection']], on=['rsID', 'study'])\\\n",
    "                      .sort_values(by='HetISq', ascending=False)[['rsID', 'intersection']].drop_duplicates()\\\n",
    "                      .dropna().intersection.apply(lambda x: intersection_mapper[x]).tolist()\n",
    "}\n",
    "\n",
    "a.intersections.update(\n",
    "  pd.Series(tmp_data.HetISq.tolist(),\n",
    "          index=a.intersections.index)\n",
    ")\n",
    "\n",
    "a._with_lines = True\n",
    "a.inclusion = np.array(inclusion_df[oc])\n",
    "a._element_size = 64\n",
    "\n",
    "scaler = colors.TwoSlopeNorm(vcenter=0,\n",
    "                    vmin=Z_LOWER,\n",
    "                    vmax=Z_UPPER)\n",
    "cmap = cm.get_cmap(CMAP_SCHEME).copy()\n",
    "cmap.set_bad(color = 'k', alpha = 1.)\n",
    "\n",
    "a.mat_color = tmp_df[['HetISq', 'rsID', 'study', 'Z']].sort_values(by='HetISq',ascending=False).set_index('study')[['Z']].apply(lambda x: scaler(x))\n",
    "a.labels = list(( tmp_data['outcome'].str.replace('log_CSF_', '') + ' - ' + tmp_data['rsID']).unique()) + ['']\n",
    "\n",
    "\n",
    "plt.subplots_adjust(left=.25, bottom=0, right=1.75, top=1.5, wspace=0, hspace=0)\n",
    "fig.suptitle('Cross-Sectional Effect Size (Z) for AD risk loci \\non log CSF AÎ²', x=1, y=1.65, fontsize=40)\n",
    "_ = a.plot(scaler, cmap, cmap_fmt='{:.2f}', fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a77f30-6591-4b75-8220-5d6d90cdf5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_grp = list(combinations(tmp_df.study.unique(), 1)) + \\\n",
    "  list(combinations(tmp_df.study.unique(), 2)) + \\\n",
    "  list(combinations(tmp_df.study.unique(), 3)) + \\\n",
    "  list(combinations(tmp_df.study.unique(), 4)) + \\\n",
    "  list(combinations(tmp_df.study.unique(), 5)) + \\\n",
    "  list(combinations(tmp_df.study.unique(), 6))\n",
    "\n",
    "new_grp = []\n",
    "for x in tmp_grp:\n",
    "  if not set(map(str.strip, x)) == {''}:\n",
    "    new_grp.append(x)\n",
    "tmp_grp = new_grp\n",
    "tmp_grp = tmp_grp[:tmp_df.rsID.nunique()]\n",
    "tmp_grp = [list(x) for x in tmp_grp]\n",
    "\n",
    "grp = from_memberships(\n",
    "  tmp_grp,\n",
    "  data = tmp_df.groupby('rsID').HetISq.max().tolist())\n",
    "#  data=dfs.groupby(['outcome', 'rsID']).OBS_CT.sum().tolist())\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "a = upsetplot.UpSet(grp)\n",
    "\n",
    "#tmp_data = dfs.groupby(['rsID', 'outcome']).OBS_CT.sum().reset_index().set_index(a.intersections.index)\n",
    "tmp_data = pd.DataFrame( tmp_df.groupby('rsID').HetISq.first().sort_values(ascending=False) ).reset_index().set_index(a.intersections.index)\n",
    "tmp_data['outcome'] = oc\n",
    "\n",
    "idx = list(filter(lambda x: x.strip() != '', tmp_data.index.names))\n",
    "intersection_mapper = {\n",
    "  'HCAD': [1 if x != 'PD' else 0 for x in idx],\n",
    "  'HCPD': [1 if x != 'ADNI-Dementia' else 0 for x in idx],\n",
    "  'PDAD': [1 if x != 'HC' else 0 for x in idx],\n",
    "  pd.NA: [0 for x in idx]\n",
    "}\n",
    "inclusion_df = {\n",
    "  oc: tmp_df.merge(df[['study', 'rsID', 'intersection']], on=['rsID', 'study'])\\\n",
    "                      .sort_values(by='HetISq', ascending=False)[['rsID', 'intersection']].drop_duplicates()\\\n",
    "                      .intersection.apply(lambda x: intersection_mapper[x]).tolist()\n",
    "}\n",
    "a.intersections.update(\n",
    "  pd.Series(tmp_data.HetISq.tolist(),\n",
    "          index=a.intersections.index)\n",
    ")\n",
    "\n",
    "a._with_lines = True\n",
    "a.inclusion = np.array(inclusion_df[oc])\n",
    "a._element_size = 64\n",
    "\n",
    "scaler = colors.TwoSlopeNorm(vcenter=0,\n",
    "                    vmin=Z_LOWER,\n",
    "                    vmax=Z_UPPER)\n",
    "cmap = cm.get_cmap(CMAP_SCHEME).copy()\n",
    "cmap.set_bad(color = 'k', alpha = 1.)\n",
    "\n",
    "a.mat_color = tmp_df[['HetISq', 'rsID', 'study', 'Z']].sort_values(by='HetISq',ascending=False).set_index('study')[['Z']].apply(lambda x: scaler(x))\n",
    "a.labels = list(( tmp_data['outcome'].str.replace('log_CSF_', '') + ' - ' + tmp_data['rsID']).unique()) + ['']\n",
    "plt.subplots_adjust(left=.25, bottom=0, right=1.75, top=1.5, wspace=0, hspace=0)\n",
    "fig.suptitle('Time-constant Effect Size (Z) for PD risk loci \\non log CSF t-Tau', x=1, y=1.65, fontsize=40)\n",
    "_ = a.plot(scaler, cmap, cmap_fmt='{:.2f}', fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb2fcf-e2d7-4e13-8bc3-8997e5b0e1de",
   "metadata": {},
   "source": [
    "## Forest Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ef34574-275f-48c0-8b06-bbbb0be3c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grep(query, fn, compressed=False):\n",
    "  cmd = f'grep -w {query} {fn}'\n",
    "  \n",
    "  if compressed:\n",
    "    cmd = 'z' + cmd\n",
    "  p = subprocess.run(cmd.split(' '),\n",
    "                     stdout=subprocess.PIPE,\n",
    "                     stderr=subprocess.STDOUT)\n",
    "  return p.stdout.decode('utf-8').strip().split('\\t')\n",
    "\n",
    "def get_query(query, fn, keep=False, metal=True):\n",
    "  q = query if metal else query[:-4]\n",
    "  data = grep(q, fn, compressed = not metal)\n",
    "  res = [query, pd.NA, pd.NA, pd.NA]\n",
    "  \n",
    "  metal_idx = [7,8,9,15,16]\n",
    "  gallop_idx = [15,16,17,8,14]\n",
    "  \n",
    "  idx = metal_idx if metal else gallop_idx\n",
    "  if data != ['']:\n",
    "    flip = False\n",
    "    if metal and data[0].split(':')[2:] != [data[1].upper(), data[2].upper()]:\n",
    "      flip = True\n",
    "      \n",
    "    if keep:\n",
    "      res = [data[0] if metal else query]\n",
    "      res += data[1:]\n",
    "    else:\n",
    "      res = [data[0] if metal else query,\n",
    "             float(data[idx[0]]) * (-1 if flip else 1)]\n",
    "      res += [ data[x] for x in idx[1:] ]\n",
    "  return res\n",
    "\n",
    "def create_SUMSTATS(data):\n",
    "  tmp_dir = 'data/metal'\n",
    "  sep='\\t'\n",
    "  for idx in range(data.shape[0]):\n",
    "    row = data.iloc[idx,:]    # ignore lambda value\n",
    "    c = row.study\n",
    "\n",
    "    with open( f'{tmp_dir}/input/{c}.input', 'w') as f:\n",
    "      f.write( sep.join( list(row.index) ) + '\\n' )\n",
    "      f.write( sep.join( map(str, list(row)) ) )  \n",
    "\n",
    "def create_METAL(outcome, cohorts):\n",
    "  tmp_dir = 'data/metal'\n",
    "  cmd = list()\n",
    "  cmd.append(f\"\"\"\\\n",
    "SCHEME STDERR\n",
    "CUSTOMVARIABLE OBS_CT_total\n",
    "LABEL OBS_CT_total as OBS_CT\n",
    "  \"\"\")\n",
    "\n",
    "  cmd.append(\"\"\"\\\n",
    "CUSTOMVARIABLE OBS_CT_REP_total\n",
    "LABEL OBS_CT_REP_total as OBS_CT_REP\n",
    "  \"\"\")\n",
    "\n",
    "  cmd.append(f\"\"\"\\\n",
    "MARKER ID\n",
    "ALLELE ALT A1\n",
    "EFFECT BETA\n",
    "PVALUE P\n",
    "STDERR SE\n",
    "  \"\"\")\n",
    "    \n",
    "  for c in cohorts:\n",
    "    fn = f'{tmp_dir}/input/{c}.input'\n",
    "    cmd.append(f'PROCESS {fn}')\n",
    "\n",
    "  cmd.append(f\"\"\"\n",
    "OUTFILE {tmp_dir}/P_ .tbl\n",
    "ANALYZE HETEROGENEITY\n",
    "QUIT\n",
    "  \"\"\")\n",
    "    \n",
    "  cmd = '\\n'.join(cmd)               \n",
    "\n",
    "  with open( f'{tmp_dir}/local.metal', 'w') as f:\n",
    "    f.write(cmd)\n",
    "    \n",
    "def conduct_METAL():\n",
    "  tmp_dir = 'data/metal'\n",
    "  fout = open( f'{tmp_dir}/metal.stdout', 'w')\n",
    "  cmd = f'metal {tmp_dir}/local.metal'\n",
    "  p1 = subprocess.run(cmd.split(),  stdout=fout)\n",
    "  \n",
    "def load_METAL():\n",
    "  tmp_dir = 'data/metal'\n",
    "  df = pd.read_csv(f'{tmp_dir}/P_1.tbl', sep=\"\\t\")\n",
    "  df['study'] = 'METAL meta-analysis'\n",
    "  return df\n",
    "  new_row = ['METAL meta-analysis',\n",
    "             df.MarkerName,\n",
    "             df.Allele1,\n",
    "             df.Allele2,\n",
    "             df.OBS_CT_total,\n",
    "             df.OBS_CT_REP_total,\n",
    "             df.Effect,\n",
    "             df.StdErr,\n",
    "             df['P-value']]\n",
    "  return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c639e54f-bac9-4ba4-ae1d-6ca0adae13f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rs16941239 chr16:86420604:T:A 91.0\n",
      "rs1582763 chr11:60254475:G:A 85.7\n",
      "rs12590654 chr14:92472511:G:A 80.7\n",
      "rs7401792 chr14:92464917:G:A 77.8\n",
      "rs56407236 chr16:90103687:G:A 76.1\n",
      "rs11771145 chr7:143413669:G:A 75.8\n",
      "rs6966331 chr7:37844191:T:C 75.5\n",
      "rs1800978 chr9:104903697:C:G 73.2\n",
      "rs871269 chr5:151052827:C:T 73.2\n",
      "rs6605556 chr6:32615322:A:G 72.3\n",
      "rs12151021 chr19:1050875:A:G 71.6\n",
      "rs6742 chr20:63743088:T:C 71.4\n",
      "rs112403360 chr5:14724304:T:A 69.8\n",
      "rs6489896 chr12:113281983:T:C 69.1\n",
      "rs5848 chr17:44352876:C:T 66.1\n",
      "rs62374257 chr5:86927378:T:C 65.8\n",
      "rs6733839 chr2:127135234:C:T 65.1\n",
      "rs10933431 chr2:233117202:G:C 64.6\n",
      "rs2526377 chr17:58332680:A:G 63.4\n",
      "rs113706587 chr5:180201150:G:A 63.1\n",
      "rs7912495 chr10:11676714:A:G 62.5\n",
      "rs6943429 chr7:7817263:T:C 61.8\n",
      "rs587709 chr19:54267597:C:T 59.5\n",
      "rs2245466 chr4:40197226:G:C 59.4\n",
      "rs3851179 chr11:86157598:T:C 59.0\n",
      "rs74685827 chr11:121482368:T:G 56.8\n",
      "rs3848143 chr15:64131307:G:A 50.2\n",
      "rs6014724 chr20:56423488:A:G 50.2\n",
      "rs7767350 chr6:47517390:C:T 47.9\n",
      "rs889555 chr16:31111250:C:T 46.5\n",
      "rs8025980 chr15:50701814:A:G 45.0\n",
      "rs10437655 chr11:47370397:G:A 43.8\n",
      "rs199515 chr17:46779275:G:C 41.6\n",
      "rs11787077 chr8:27607795:T:C 41.1\n",
      "rs13237518 chr7:12229967:C:A 39.7\n",
      "rs73223431 chr8:27362470:C:T 36.0\n",
      "rs2154481 chr21:26101558:C:T 34.7\n",
      "rs76928645 chr7:54873635:C:T 31.2\n",
      "rs6846529 chr4:11023507:C:T 21.2\n",
      "rs16824536 chr3:155069722:G:A 19.1\n",
      "rs4277405 chr17:63471557:C:T 14.5\n",
      "rs7908662 chr10:122413396:A:G 14.2\n",
      "rs1140239 chr16:30010081:C:T 9.9\n"
     ]
    }
   ],
   "source": [
    "base = Path('METAL')\n",
    "het_data = []\n",
    "\n",
    "for idx, row in df[['rsID', 'MarkerName', 'HetISq']][df.HetISq > 0]\\\n",
    "                  .drop_duplicates().sort_values(by='HetISq', ascending=False)\\\n",
    "                  .iterrows():\n",
    "  rsid = row.rsID\n",
    "  query = row.MarkerName\n",
    "  print(rsid, query, row.HetISq)\n",
    "  \n",
    "  fn = base / f'meta_analysis/lt/{oc}/Pi_HC1.tbl'\n",
    "  cohort = 'HC'\n",
    "  res = get_query(query, fn) + [cohort, oc, rsid]\n",
    "  het_data.append(res)\n",
    "  cohort = 'PD'\n",
    "  fn = base / f'meta_analysis/lt/{oc}/Pi_PD.all1.tbl'\n",
    "  res = get_query(query, fn) + [cohort, oc, rsid]\n",
    "  het_data.append(res)\n",
    "  fn = base / f'raw_summary_stats/lt/EUR_ADNI-Dementia_allchr.{oc}.gallop.gz'\n",
    "  cohort = 'ADNI-Dementia'\n",
    "  res = get_query(query, fn, metal=False) + [cohort, oc, rsid]\n",
    "  het_data.append(res)\n",
    "\n",
    "\n",
    "tmp_df = pd.DataFrame(het_data, \n",
    "           columns=['ID', 'BETA', 'SE', 'P', 'OBS_CT', 'OBS_CT_REP', 'study', 'outcome', 'rsID'])\n",
    "tmp_df = tmp_df.dropna()\n",
    "tmp_df['BETA'] = tmp_df.BETA.astype(float)\n",
    "tmp_df['SE'] = tmp_df.SE.astype(float)\n",
    "tmp_df['OBS_CT'] = tmp_df.OBS_CT.astype(float)\n",
    "tmp_df['OBS_CT_REP'] = tmp_df.OBS_CT_REP.astype(float)\n",
    "tmp_df['P'] = tmp_df.P.astype(float).apply(lambda x: '{:.3E}'.format(x) if x < 0.01 else '{:.3f}'.format(x))\n",
    "tmp_df['ALT'] = tmp_df.ID.str.split(':').apply(lambda x: x[2])\n",
    "tmp_df['A1'] = tmp_df.ID.str.split(':').apply(lambda x: x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be8037e8-5bc0-4f82-8864-bfb705acc63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for study in tmp_df.study.unique():\n",
    "  tmp_df[tmp_df.study==study].to_csv(f'figures/het_analysis/data/{study}.{oc}.input',\n",
    "                                    sep='\\t', index=False)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1eae0e1-fe0e-4d1b-b22a-f7bf04141151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rs6943429 chr7:7817263:T:C 85.3\n",
      "rs7157106 chr14:105761758:A:G 83.9\n",
      "rs1358782 chr20:413334:A:G 83.3\n",
      "rs587709 chr19:54267597:C:T 78.7\n",
      "rs2830489 chr21:26775872:C:T 74.9\n",
      "rs5848 chr17:44352876:C:T 74.6\n",
      "rs16941239 chr16:86420604:T:A 74.0\n",
      "rs7068231 chr10:60025170:T:G 71.6\n",
      "rs679515 chr1:207577223:T:C 70.8\n",
      "rs7225151 chr17:5233752:G:A 69.7\n",
      "rs17020490 chr2:37304796:T:C 69.2\n",
      "rs10437655 chr11:47370397:G:A 65.2\n",
      "rs72777026 chr2:9558882:A:G 65.0\n",
      "rs6586028 chr10:80494228:C:T 62.1\n",
      "rs889555 chr16:31111250:C:T 57.5\n",
      "rs6014724 chr20:56423488:A:G 55.8\n",
      "rs3822030 chr4:993555:G:T 55.0\n",
      "rs602602 chr15:58764824:T:A 53.9\n",
      "rs6605556 chr6:32615322:A:G 53.6\n",
      "rs8025980 chr15:50701814:A:G 52.6\n"
     ]
    }
   ],
   "source": [
    "base = Path('METAL')\n",
    "\n",
    "\n",
    "for idx, row in df[['rsID', 'MarkerName', 'HetISq']][df.HetISq > 0]\\\n",
    "                  .drop_duplicates().sort_values(by='HetISq', ascending=False)\\\n",
    "                  .head(20).iterrows():\n",
    "  het_data = []\n",
    "  rsid = row.rsID\n",
    "  query = row.MarkerName\n",
    "  print(rsid, query, row.HetISq)\n",
    "  \n",
    "  fn = base / f'meta_analysis/lt/{oc}/Pi_HC1.tbl'\n",
    "  cohort = 'HC'\n",
    "  res = get_query(query, fn) + [cohort, oc, rsid]\n",
    "  het_data.append(res)\n",
    "  cohort = 'PD'\n",
    "  fn = base / f'meta_analysis/lt/{oc}/Pi_PD.all1.tbl'\n",
    "  res = get_query(query, fn) + [cohort, oc, rsid]\n",
    "  het_data.append(res)\n",
    "  fn = base / f'raw_summary_stats/lt/EUR_ADNI-Dementia_allchr.{oc}.gallop.gz'\n",
    "  cohort = 'ADNI-Dementia'\n",
    "  res = get_query(query, fn, metal=False) + [cohort, oc, rsid]\n",
    "  het_data.append(res)\n",
    "  \n",
    "  \n",
    "  tmp_df = pd.DataFrame(het_data, \n",
    "             columns=['ID', 'BETA', 'SE', 'P', 'OBS_CT', 'OBS_CT_REP', 'study', 'outcome', 'rsID'])\n",
    "\n",
    "  tmp_df = tmp_df.dropna()\n",
    "  tmp_df['BETA'] = tmp_df.BETA.astype(float)\n",
    "  tmp_df['SE'] = tmp_df.SE.astype(float)\n",
    "  tmp_df['OBS_CT'] = tmp_df.OBS_CT.astype(float)\n",
    "  tmp_df['OBS_CT_REP'] = tmp_df.OBS_CT_REP.astype(float)\n",
    "  tmp_df['P'] = tmp_df.P.astype(float).apply(lambda x: '{:.3E}'.format(x) if x < 0.01 else '{:.3f}'.format(x))\n",
    "  tmp_df['ALT'] = tmp_df.ID.str.split(':').apply(lambda x: x[2])\n",
    "  tmp_df['A1'] = tmp_df.ID.str.split(':').apply(lambda x: x[3])\n",
    "  \n",
    "  create_SUMSTATS(tmp_df)\n",
    "  create_METAL(oc, ['HC', 'PD', 'ADNI-Dementia'])\n",
    "  conduct_METAL()\n",
    "  meta_res = load_METAL()\n",
    "  meta_res = meta_res.rename(columns= {'Effect': 'BETA',\n",
    "                                       'StdErr': 'SE',\n",
    "                                       'P-value': 'P',\n",
    "                                       'OBS_CT_total': 'OBS_CT',\n",
    "                                       'OBS_CT_REP_total': 'OBS_CT_REP'})\n",
    "  \n",
    "  title = 'log CSF AÎ² Time-constant - {} ({})'.format(rsid, query)\n",
    "  #title = 'log CSF t-tau Time-constant - {} ({})'.format(rsid, query)\n",
    "\n",
    "  \n",
    "  output_file(filename=f'figures/het_analysis/{oc}.time-const.{rsid}.het.html',\n",
    "              title=title)\n",
    "  save(\n",
    "    forest_plot(pd.concat([meta_res, tmp_df]),\n",
    "           meta=meta_res,\n",
    "           query={'effect': 'Intercept',\n",
    "                  'outcome': oc},\n",
    "           show_pval=True,\n",
    "           title=title))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python/3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
